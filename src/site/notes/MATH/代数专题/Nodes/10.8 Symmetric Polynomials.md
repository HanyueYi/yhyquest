---
{"dg-publish":true,"permalink":"/MATH/代数专题/Nodes/10.8 Symmetric Polynomials/","dgPassFrontmatter":true}
---


# Some Symmetric Polynomials: p, e and h

> [!definition]
> A polynomial in $m$ variables is called *symmetric* if it is stable under all permutations $\sigma\in S_m$, that is, $\sigma(p(x_1,\cdots,x_n))=p(x_{\sigma(1)},\cdots,x_{\sigma(n)})$.
> 
> The algebra of symmetric polynomials is denoted by $\Lambda_m$. 

**Examples.** They are symmetric polynomials.
- $p_n=x_1^n+\cdots+x_m^n$ for all $n\in \mathbb{N}_+$, where the corresponding generated function is $P(t)=\sum_{n=1}^\infty p_nt^{n-1}=\sum_{i=1}^m\dfrac{x_i}{1-x_it}$. 
- The elementary symmetric polynomials $e_n=\sum_{i_1<\cdots<i_n}x_{i_1}\cdots x_{i_n}$ for $n\geqslant 1$. Furthermore, define $e_0=1$. Note that $e_n=0$ when $n\geqslant m$, and the corresponding generated function is $E(t)=\sum_{n=0}^t e_nt^n=\prod_{i=1}^m(1+x_it)$.
- The complete symmetric polynomials $h_n=\sum_{i_1\leqslant\cdots\leqslant i_n}x_{i_1}\cdots x_{i_n}$ for $n\geqslant 1$. We also define $h_0=1$, and the corresponding generated function is $H(t)=\sum_{n=0}^\infty h_nt^n=\prod_{i=1}^\infty\dfrac{1}{1-x_i t}$. 

Since $H(t)E(-t)=1$, we have $\sum_{n=0}^\infty(-1)^re_rh_{n-r}=0$. Next, notice that $p(t)=d/dt\ln\prod_{i=1}^\infty \frac{1}{1-x_it}$, then 

$$P(t)=d/dt\ln(H(t))=\frac{H'(t)}{H(t)}.$$
{ #skztji}


Similarly $P(-t)=E'(t)/E(t)$. It deduces the Newton identity.

> [!lemma] the Newton identity
> With the definition above, we have $nh_n=\sum_{r=1}^\infty p_rh_{n-r}$ and $ne_n=\sum_{r=1}^n(-1)^{r-1}p_re_{n-r}$. 
{ #sbpikb}


> [!proposition]
> If $A$ is an $m \times m$ matrix, then the coefficients of the characteristic polynomial $\operatorname{det}(A-t I)$ are given by the elementary symmetric functions of the eigenvalues $\left\{\lambda_1, \lambda_2, \ldots, \lambda_m\right\}$, with alternating signs $\pm 1$ depending on the degree of each term. The power sums of the eigenvalues coincide with $\operatorname{tr}A^n=\lambda_1^n+\cdots+\lambda_m^n$.

**Remark.** There is some connection between symmetric polynomials and the rooted solution of polynomial equations. See [here](https://math.stackexchange.com/a/96329/1445401). When the degree of polynomial is less than or equal to $4$, the roots can be obtained by operating the coefficients by combination of adding, subtracting, multiplication, division, and taking roots. 

# Schur Polynomial

> [!definition]
> Suppose $\lambda=(\lambda_1,\cdots,\lambda_m)$ is a partition of $n$ of length at most $m$. The Schur polynomial $\mathscr s_\lambda$ is defined by 
> 
> $$\mathscr s_\lambda=\frac{\left|\begin{matrix}
x_1^{\lambda_1+m-1} & \cdots  & x_m^{\lambda_1+m-1} \\
x_1^{\lambda_2+m-2} & \cdots & x_m^{\lambda_2+m-2} \\
\vdots &  & \vdots \\
x_1^{\lambda_m} & \cdots & x_m^{\lambda_m}
\end{matrix}\right|}{\left|\begin{matrix}
x_1^{m-1} & \cdots  & x_m^{m-1} \\
x_1^{m-2} & \cdots & x_m^{m-2} \\
\vdots &  & \vdots \\
1 & \cdots & 1
\end{matrix}\right|}.$$
> 
>
{ #i6y60w}


For example, when $\lambda=(n)$, $\mathscr s_\lambda=h_n$. When $\lambda=1^n$ and $m=n$, $\mathscr s_\lambda=x_1\cdots x_n$. We can prove them by computing directly, or by the following proposition.

> [!proposition] Jacobi-Trudi formula
> We have 
> 
> $$\mathscr s_\lambda=\left|\begin{matrix}h_{\lambda_1} & h_{\lambda_1+1} & \cdots & h_{\lambda_1+m-1} \\ h_{\lambda_2-1} & h_{\lambda_2} & \cdots & h_{\lambda_2+m-2}\\ \vdots & \vdots &  & \vdots \\ h_{\lambda_m-m+1} & h_{\lambda_m-m+2} & \cdots & h_{\lambda_m}\end{matrix}\right|.$$
{ #jrsjoc}


`\begin{proof}`
Let $\alpha=(\alpha_1,\cdots,\alpha_m)$ be a composition. Put 

$$A_\alpha=\begin{bmatrix} x_1^{\alpha_1} & \cdots & x_m^{\alpha_1} \\
\vdots  &  & \vdots \\
x_1^{\alpha_m} & \cdots & x_m^{\alpha_m} \end{bmatrix}\mbox{ and }H_\alpha =\begin{bmatrix} h_{\alpha_1-m+1} & \cdots & h_{\alpha_1} \\
\vdots  &  & \vdots \\
h_{\alpha_m-m+1} & \cdots & h_{\alpha_m}\end{bmatrix}.$$

Consider the elementary symmetric polynomial $e_n^{(k)}$ in variables $x_1,\cdots,\hat x_k,\cdots,x_m$ with $1\leqslant k\leqslant m$ and write them to $m\times m$ matrix 

$$M=\begin{bmatrix} (-1)^{m-1}e_{m-1}^{(1)} & \cdots & (-1)^{m-1}e_{m-1}^{(m)} \\
\vdots &  & \vdots \\
(-1)e_1^{(1)} & \cdots & (-1)e_1^{(m)} \\
1 & \cdots & 1\end{bmatrix}.$$

We claim that $A_\alpha=H_\alpha M$. Consider generating function for $e_n^{(k)}$ 

$$E^{(k)}(t)=\sum_{n=0}^{m-1}e_n^{(k)}t^n=\prod_{i\neq k}(1+x_it),$$

then by $H(t)E(-t)=1$ there is 

$$H(t)E^{(k)}(t)=\frac{1}{1-x_kt}=1+x_kt+\cdots+(x_kt)^\ell+\cdots.$$

Take the coefficient of $t^{\alpha_i}$ and we have 

$$\sum_{j=1}^m h_{\alpha_i-m+j}(-1)^{m-j}e_{m-j}^{(k)}=x_k^{\alpha_i}\tag{*}.$$

With $(*)$, we can prove the claim. Thus, $\det A_\alpha=\det H_\alpha\det M$ and so $\det H_\alpha=\det A_\alpha/\det M$. For $\delta=(m-1,m-2,\cdots,1,0)$, the matrix $H_\delta=1$ and so $\det A_\delta=\det M$. It follows that 

$$\det H_\alpha=\frac{\det A_\alpha}{\det A_\delta}=\mathscr s_\lambda,$$

where $\alpha=\lambda+\delta=(\lambda_1+m-1,\lambda_2+m-2,\cdots,\lambda_m)$. Now we finish the proof.
`\end{proof}`


We have a more direct formula for $\mathscr s_\lambda$ by considering a generalized tableau of shape $\lambda$. 


> [!definition] 
> We place numbers $\{1,\cdots,n\}$ into tableau $\lambda\vdash n$ allowing repetition. Such tableau is called *semistandard* if rows are weakly increasing (not decreasing) while columns are strictly increasing sequences. 
> 
> If $T$ is semistandard with $\{1,\cdots,n\}$, set 
> 
> $$\mathscr x^T=\prod_{i=1}^n(x_i)^{\operatorname{number of occurrence of }i \in T}.$$

> [!proposition]
> For $\lambda\vdash n$ with length $m$, the Schur polynomial in $m$ variables is $\mathscr s_\lambda=\sum_T \mathscr x^T$.

# Basis of $\Lambda_m$

Now we consider algebra of symmetric polynomials in $m$ variables.

First define $\Lambda_m^n=\mathrm{span}\{\mathscr s_\lambda:\lambda\vdash n\}$, the subspace of homogeneous symmetric polynomial of degree $n$, and then define $\Lambda_m=\oplus_{n\geqslant 0}\Lambda_m^n$. 

For example, when $m=3$, we have $\Lambda_m^0=\mathrm{span}\{1\}$, $\Lambda_m^1=\mathrm{span}\{x_1+x_2+x_3\}$,

$$\begin{aligned}
\Lambda_m^2&=\mathrm{span}\{\mathscr s_{(1,1)},\mathscr s_{(2,0)}\}=\mathrm{span}\{x_1x_2+x_1x_3+x_2x_3,x_1^2+x_2^2+x_3^2+x_1x_2+x_1x_3+x_2x_3\}, \\
\Lambda_m^3&=\mathrm{span}\{s_{(3)},s_{(2,1)},s_{(1,1,1)}\}=\mathrm{span}\{x_1x_2x_3,x_1^2x_2+x_1^2x_3+x_1x_2^2+2x_1x_2x_3+x_1x_3^2+x_2^2x_3+x_2x_3^2,h_3\}.
\end{aligned}$$

> [!proposition]
> $\mathscr s_\lambda$ is a basis for $\Lambda_m$ for all $\lambda\vdash n$ with $n\geqslant 0$. (For a fixed $n$, $\lambda\vdash n$ running through all partitions $\{\mathscr s_\lambda:\lambda\vdash n\}$ is a basis for $\Lambda_m^k$.)

Then we introduce a few families of symmetric polynomials for algebra $\Lambda_m$. 
- For any partition $\lambda=(\lambda_1,\cdots,\lambda_m)$, define $p_\lambda=p_{\lambda_1}\cdots p_{\lambda_m}$, $e_\lambda=e_{\lambda_1}\cdots e_{\lambda_m}$ and $h_\lambda=h_{\lambda_1}\cdots h_{\lambda_m}$. For example, when $m=3$ and $n=2$, $p_{(2)}=p_2=x_1^2+x_2^2+x_3^2$ and $p_{(1,1)}=p_1p_1=(x_1+x_2+x_3)^2$, $e_{(2)}=e_2=x_1x_2+x_2x_3+x_1x_3$ and $e_{(1,1)}=e_1e_1=(x_1+x_2+x_3)^2$, $h_{(2)}=h_2=x_1^2+x_2^2+x_3^2+x_1x_2+x_1x_3+x_2x_3$ and $h_{(1,1)}=h_1h_1=(x_1+x_2+x_3)^2$. 
- Let $\lambda=(\lambda_1,\cdots,\lambda_m)$. Another family of symmetric polynomials is defined as the sum of all different monomials obtained from $x_1^{\lambda_1}\cdots x_m^{\lambda_m}$ under permutation. For example, when $m=3$ and $n=2$, we have $m_{(2)}=x_1^2+x_2^2+x_3^2$, $m_{(1,1)}=2(x_1x_2+x_2x_3+x_1x_3)$. 

> [!theorem]
> Suppose that $\lambda$ runs over all partitions of $n$ of length at most $m$. Then each of the following families is basis of the space $\Lambda_m^n$: 
> 
> $$m_\lambda,\mathscr s_\lambda,e_{\lambda'},h_{\lambda'},p_{\lambda'},e_\lambda,h_\lambda,p_\lambda$$
> 
> In particular, $\dim \Lambda_m^n=p(n)$ with $m\geqslant n$. 

`\begin{proof}`
We first prove $\{m_\lambda:\lambda\vdash n\}$ is a basis. Note that they are linearly independent: If $\lambda\vdash n$ and $\mu\vdash n$ with $\lambda\neq \mu$, then $m_\lambda$ and $m_\mu$ have no common monomials. Hence, if $\sum_i \alpha_i m_{\lambda_i}=0$, then $\alpha_i=0$. Suppose $f\in \Lambda_m^n$, and we aim to show $f\in\mathrm{span}\{m_\lambda:\lambda\vdash n\}$. We do induction on the order of the element. Take the greatest element $x_1^{n_1}\cdots x_m^{n_m}$ in [[MATH/代数专题/Nodes/10.3 Ordering of Partitions#^0ujtr4\|lexicographic]] order with $\mu=(n_1,\cdots,n_m)$. Then $f-m_\mu\in\Lambda_m^n$ and $f-m_\mu<f$. By induction, $f$ can be written as linear combination of $\{m_\lambda:\lambda\vdash n\}$. 

To show $\mathscr s_\lambda$ form a basis, define $A_m$ as the space of skew-symmetric polynomials in $x_1,\cdots,x_m$. We say $g$ is skew-symmetric if $(ij)g=-g$. It is easy to see that any skew-symmetric polynomials is divisible by $\prod_{1\leqslant i<j\leqslant m}(x_i-x_j)$. We have that $\varphi:\Lambda_m\to A_m,f\mapsto f\prod_{1\leqslant i<j\leqslant m}(x_i-x_j)$, where $\varphi$ defines an isomorphism between $\Lambda_m$ and $A_m$. One can [check](https://math.uchicago.edu/~may/REU2020/REUPapers/Graham.pdf?utm_source=chatgpt.com) that basis of $A_m$ can be given by 

$$b_\lambda=\left|\begin{matrix}
x_1^{\lambda_1+m-1} & \cdots & x_m^{\lambda_1+m-1} \\
x_1^{\lambda_2+m-2} & \cdots & x_m^{\lambda_2+m-2} \\
\vdots &  & \vdots \\
x_1^{\lambda_m} & \cdots & x_m^{\lambda_m}
\end{matrix}\right|$$

where $\lambda$ runs over through partitions of $n$ of length $\leqslant m$. Notice that $\mathscr s_\lambda=b_\lambda/\prod_{1\leqslant i<j\leqslant m}(x_i-x_j)$ and $\varphi(\mathscr s_\lambda)=b_\lambda$. Furthermore, as $\dim\Lambda_m^n=\dim A_m^{n+{m\choose 2}}$, it deduces that $\mathscr s_\lambda$ form a basis. Moreover, recall that [[MATH/代数专题/Nodes/10.4 Complete List of Irreducible Modules#^i3a31d\|10.4 Complete List of Irreducible Modules#^i3a31d]] and similarly we have $\mathscr s_\mu=\sum_{\lambda}\kappa_{\mu\lambda}m_\lambda=m_\mu+\sum_{\lambda\neq \mu}\kappa_{\mu\lambda}m_\lambda$, and it yields that $\left\langle \mathscr s_\mu:\mu\vdash n\right\rangle=\left\langle m_\mu:\mu\vdash n\right\rangle=\Lambda_m^n$. 

To show $e_{\lambda'}$ form a basis, suppose that $\lambda\vdash n$ with $\lambda=(\lambda_1,\cdots,\lambda_k)$ and $\lambda'=(\rho_1,\cdots,\rho_k)$, and then 

$$e_{\lambda'}=e_{\rho_1}\cdots e_{\rho_k}=(x_1\cdots x_{\rho_1}+\cdots)(x_1\cdots x_{\rho_2}+\cdots)\cdots(x_1\cdots x_{\rho_k}+\cdots)=x_1^{\lambda_1}\cdots x_m^{\lambda_m}+\cdots+\mbox{smaller terms}=m_\lambda+\sum_{\mu\lhd\lambda}m_\mu.$$

For $h_\lambda$, we can do it by [[MATH/代数专题/Nodes/10.8 Symmetric Polynomials#^jrsjoc\|#^jrsjoc]], as it deduces that $\left\langle \mathscr s_\lambda:\lambda\vdash n\right\rangle\subseteq\left\langle h_\lambda:\lambda\vdash n\right\rangle$. 
`\end{proof}`


> [!definition]
> For each $m$ we have 
> 
> $$\Lambda_{m+1}\to \Lambda_m,f(x_1,\cdots,x_m,x_{m+1})\mapsto f(x_1,\cdots,x_m,0)$$
> 
> Define a [projective limit](https://en.wikipedia.org/wiki/Inverse_limit) algebra $\Lambda=\oplus_{n\geqslant 0}\Lambda^n$ where $f=(f_m:m\geqslant 0)$ is a sequence of $f_m\in\Lambda_m^n$ and $f_{m+1}\mapsto f_m$ for all $m\geqslant 0$. 

Then $f$ is a formal series in infinitely many variables, and there are several examples:
- $p_n=\sum_{i=1}^\infty x_i^n$
- $e_n=\sum_{1\leqslant i_1<\cdots< i_n}x_{i_1}\cdots x_{i_n}$
- $h_n=\sum_{1\leqslant i_1\leqslant\cdots\leqslant i_n}x_{i_1}\cdots x_{i_n}$

For more details of projective limit, see page 489 of [[Algebra chapter 0 - 2009 - Aluffi.pdf]].
The sequence of Schur polynomials $\mathscr s_\lambda$ defines Schur functions, and the sequence of $m_\lambda$ defines the monomial symmetric functions.

> [!lemma]
> We have
> 
> $$h_n=\sum_{\lambda\vdash n}\frac{p_\lambda}{z_\lambda}\mbox{ and }e_n=\sum_{\lambda\vdash n}\frac{\epsilon_\lambda p_\lambda}{z_\lambda}$$
> 
> where $\lambda=(1^{m_1},\cdots,n^{m_n})$, $z_\lambda=1^{m_1}m_1! \cdots n^{m_n}m_n!$ and $\epsilon_\lambda=(-1)^{n-\ell(\lambda)}$.
{ #dbe3dc}


`\begin{proof}`
Recall that 


<div class="transclusion internal-embed is-loaded"><a class="markdown-embed-link" href="/math//nodes/10-8-symmetric-polynomials/#skztji" aria-label="Open link"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon lucide-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a><div class="markdown-embed">



$P(t)=d/dt\ln(H(t))=\frac{H'(t)}{H(t)}.$ 

</div></div>


It follows 

$$H(t)=\exp(\sum_{k=1}^\infty \frac{p_kt^k}{k})=\prod_{k=1}^\infty\exp(\frac{p_kt^k}{k})=\left( 1+p_1t+\frac{(p_1t)^2}{2!}+\cdots\right)\left( 1+\frac{p_2t}{2}+\frac{(p_1t)^2}{2^2\cdot2!}+\cdots\right)\left( 1+\frac{p_3t}{3}+\frac{(p_3t)^2}{3^2\cdot 2!}+\cdots\right)$$

and it deduces what we desire. Similarly we can prove $e_n=\sum_{\lambda\vdash n}{\epsilon_\lambda p_\lambda}/{z_\lambda}$.
`\end{proof}`


> [!proposition] orthogonal properties
> For two families of variables $x=(x_1,\cdots,x_k,\cdots)$ and $y=(y_1,\cdots,y_k,\cdots)$, we have the following properties
> 
> $$\begin{aligned}
> \prod_{i,j\geqslant 1}(1-x_iy_j)^{-1}&=\sum_{\lambda}z_\lambda^{-1}p_\lambda(x)p_\lambda(y),\\
> \prod_{i,j\geqslant 1}(1-x_iy_j)^{-1}&=\sum_\lambda h_\lambda(x)m_\lambda(y)=\sum_\lambda m_\lambda(x)h_\lambda(y)=\sum_\lambda \mathscr s_\lambda(x)\mathscr s_\lambda(y).
> \end{aligned}$$
> 

`\begin{proof}`
By [[MATH/代数专题/Nodes/10.8 Symmetric Polynomials#^dbe3dc\|#^dbe3dc]], we have

$$\prod_{i\geqslant 1}(1-x_it)^{-1}=\sum_\lambda z_\lambda^{-1}p_\lambda t^{|\lambda|}\tag{*}$$

where $|\lambda|$ is the length of $\lambda$. 

Note that $\sum_{i,j}(x_iy_j)^k=p_k(x)p_k(y)$. Use $(*)$ for $x_iy_j$ and set $t=1$, we have 

$$\prod_{i,j\geqslant 1}(1-x_i y_j)^{-1}=\sum_\lambda z_\lambda^{-1} p_\lambda(x_iy_j)=\sum_\lambda z_\lambda^{-1}p_\lambda(x_i)p_\lambda(y_j).$$

Equip $\Lambda$ with the following form $\left\langle h_\lambda, m_\mu\right\rangle=\delta_{\lambda\mu}$, and it deduces that $\left\langle p_\lambda,p_\mu\right\rangle=z_\lambda \delta_{\lambda\mu}$ and $\left\langle \mathscr s_\lambda,\mathscr s_\mu\right\rangle=\delta_{\lambda\mu}$. With respect to this inner product, $\{\mathscr s_\lambda:\lambda\vdash n\}$ is orthonormal basis of $\Lambda^n$. 
`\end{proof}`


